{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heisenberg/anaconda3/envs/iit/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/heisenberg/anaconda3/envs/iit/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/heisenberg/anaconda3/envs/iit/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "01/21/2023 10:40:30 PM: [ Loading faiss with AVX2 support. ]\n",
      "01/21/2023 10:40:30 PM: [ Loading faiss with AVX2 support. ]\n",
      "01/21/2023 10:40:30 PM: [ Successfully loaded faiss with AVX2 support. ]\n",
      "01/21/2023 10:40:30 PM: [ Successfully loaded faiss with AVX2 support. ]\n"
     ]
    }
   ],
   "source": [
    "import allennlp\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from allennlp.modules.span_extractors.endpoint_span_extractor import EndpointSpanExtractor\n",
    "from tqdm import tqdm\n",
    "from utils.drqa.DocRanker import docranker_utils\n",
    "from utils.drqa.DocRanker.tokenizer import CoreNLPTokenizer\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_path = \"data-dir/test/sqlite_con-tfidf-ngram=3-hash=33554432-tokenizer=corenlp.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ , metadata = docranker_utils.load_sparse_csr(tfidf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function to read a json file line by line as a dictionary\n",
    "def read_jsonl(file_path):\n",
    "\twith open(file_path, 'r') as f:\n",
    "\t\tdata = []\n",
    "\t\tfor line in f:\n",
    "\t\t\tdata.append(json.loads(line))\n",
    "\treturn data\n",
    "\n",
    "test_data = read_jsonl(\"data-dir/test/contexts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for id in metadata['doc_dict'][1]:\n",
    "\tfor point in test_data:\n",
    "\t\tif point['id'] == id:\n",
    "\t\t\ttexts.append(point['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/21/2023 10:40:47 PM: [ Load pretrained SentenceTransformer: sentence-transformers/multi-qa-distilbert-cos-v1 ]\n",
      "01/21/2023 10:40:47 PM: [ Load pretrained SentenceTransformer: sentence-transformers/multi-qa-distilbert-cos-v1 ]\n",
      "01/21/2023 10:40:48 PM: [ Use pytorch device: cuda ]\n",
      "01/21/2023 10:40:48 PM: [ Use pytorch device: cuda ]\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/multi-qa-distilbert-cos-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 273/273 [00:15<00:00, 17.88it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.encode(texts, show_progress_bar=True, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save embeddings\n",
    "torch.save(embeddings, 'data-dir/test/sentence_transformer_embeddings_multi-qa-distilbert-cos-v1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8726, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the embeddings\n",
    "embeddings = embeddings / embeddings.norm(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the embeddings to a numpy array\n",
    "embeddings = embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the embeddings\n",
    "np.save(\"data-dir/test/sentence_transformer_embeddings_multi-qa-distilbert-cos-v1.npy\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to tokenize this text and return its embeddings\n",
    "def get_embeddings(texts):\n",
    "\tencoded_input = tokenizer(texts, padding=False, truncation=True, return_tensors='pt').to(\"cuda\")\n",
    "\twith torch.no_grad():\n",
    "\t\tmodel_output = model(**encoded_input)\n",
    "\treturn model_output.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_cls_embedding(texts):\n",
    "\treturn get_embeddings(texts)[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an endpoint span extractor\n",
    "span_extractor = EndpointSpanExtractor(input_dim=768, combination=\"x,y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function to get the endpoint span embeddings using allennlp endpoint span extractor\n",
    "def get_endpoint_span_embeddings(text):\n",
    "\tembeddings = get_embeddings(text)\n",
    "\t# extract the span from start till end, do it without gradient calculation\n",
    "\twith torch.no_grad():\n",
    "\t\tspan_embeddings = span_extractor(embeddings, torch.tensor([[0, embeddings.shape[1]-1]]))\n",
    "\treturn span_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8726/8726 [01:05<00:00, 133.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# create a torch tensor to store the embeddings of all the texts using the get_endpoint_span_embeddings function\n",
    "all_embeddings = []\n",
    "for text in tqdm(texts):\n",
    "\tall_embeddings.append(return_cls_embedding(text))\n",
    "all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "# # load the saved span embeddings\n",
    "# all_embeddings = torch.load(\"data-dir/test/endpoint_span_embeddings.pt\")\n",
    "import numpy as np\n",
    "all_embeddings = all_embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the embeddings\n",
    "all_embeddings = all_embeddings / np.linalg.norm(all_embeddings, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save these embeddings as a numpy array\n",
    "np.save(\"data-dir/test/cls_embeddings_normalised.npy\", all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8726, 1536)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8726, 1664)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 1664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(d) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m D, I \u001b[39m=\u001b[39m index\u001b[39m.\u001b[39;49msearch(all_embeddings[\u001b[39m0\u001b[39;49m], \u001b[39m4\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/iit/lib/python3.10/site-packages/faiss/__init__.py:307\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_search\u001b[0;34m(self, x, k, D, I)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreplacement_search\u001b[39m(\u001b[39mself\u001b[39m, x, k, D\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, I\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    283\u001b[0m     \u001b[39m\"\"\"Find the k nearest neighbors of the set of vectors x in the index.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \n\u001b[1;32m    285\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[39m        When not enough results are found, the label is set to -1\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 307\u001b[0m     n, d \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\n\u001b[1;32m    308\u001b[0m     \u001b[39massert\u001b[39;00m d \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md\n\u001b[1;32m    310\u001b[0m     \u001b[39massert\u001b[39;00m k \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "D, I = index.search(all_embeddings[0], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1664,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1664)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embeddings[0:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.3770927 , -0.35221133, -0.33465827, ...,  0.039351  ,\n",
       "       -0.07162095,  0.00525754], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.     , 168.50912, 199.08183, 202.2987 ], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  38, 140, 144])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random numpy array of size 145\n",
    "random_array = np.random.randint(0, 145, 145)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 60,  10, 139,  65])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_array[I]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd66cb49a841fcb800c6001c710cf6a2c3702cb7d0b679bebfede6c97481890d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
