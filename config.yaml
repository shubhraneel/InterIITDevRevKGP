task: qa
seed: 3407
wandb_path: run-distillation
fewshot_qa: False
inference_device: cuda
train: True
inference: True
save_model_optimizer: True
load_model_optimizer: False
load_path: run-distillation
teacher_load_path: v2-bert-experiments-24
use_drqa: True
create_drqa_tfidf: False
top_k: 5
quantize: False
ONNX: False
prepare_distillation: False
run_distillation: True

data:
  train_data_path: data-dir/train/df_train.pkl
  val_data_path: data-dir/val/df_val.pkl
  test_data_path: data-dir/test/df_test.pkl
  train_batch_size: 16
  val_batch_size: 2
  apply_aliasing: False
  pad_on_right: True
  max_length: 512
  doc_stride: 0
  tokenizer_batch_size: 32

training:
  epochs: 10
  optimizer: adam
  lr: 0.00002
  sched_function: none
  sched_func_params: {}
  sched_params:
    monitor: val_loss
  evaluate_every: 1

model:
  model_path: google/bert_uncased_L-4_H-128_A-2
  teacher_model_path: google/bert_uncased_L-12_H-768_A-12
  params: {}
  non_pooler: False
  dim: 128
  two_step_loss: False
